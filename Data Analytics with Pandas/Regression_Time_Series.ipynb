{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISOM 352 Applied Data Analytics with Coding\n",
    "## Logistic Regression and Time Series Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and import the library \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the context of Titanic, what's the main question?\n",
    "### Q1: *Who `survived` Titanic tragedy?* Q2: \"Women and Children first?\"\n",
    "- Step 1: Descriptive Analytics (numerically and graphically)\n",
    "- Step 2: Simple logistic regression (as baseline)\n",
    "- Step 3: Develop the best logistic regression model \n",
    "- Step 4: Intepret the impact of the variables \n",
    "- Step 5: Make prediction and evaluate the accuracy. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Preprocessing the data\n",
    "- proper data type\n",
    "- missing values\n",
    "- dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Load and clean the dataset\n",
    "df = pd.read_csv('titanic.csv')\n",
    "df.info()\n",
    "\n",
    "# Explore and process missing values\n",
    "# 0.1 Cast object to category\n",
    "for col in df.columns[df.dtypes == 'object']:\n",
    "    df[col] = df[col].astype('category')\n",
    "\n",
    "df.info()\n",
    "\n",
    "# 0.2 Explore missing values\n",
    "df = df.dropna(axis=0)\n",
    "df.isna().sum()\n",
    "\n",
    "# 03. Data manipulation: dummy variables for categorical variables are necessary for regression purpose\n",
    "df_dummy = pd.get_dummies(data=df, drop_first=True, dtype='int8')\n",
    "df_dummy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Descriptive Analytics\n",
    "### Step 2-3: Logistic Regression\n",
    "### Step 4: Backward elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Mutiple Regression model \n",
    "from scipy import stats\n",
    "\n",
    "# Specify the predictor X and the target Y\n",
    "y = df_dummy['survived']\n",
    "x = df_dummy.drop(columns='survived')\n",
    "\n",
    "# Add constant to the features\n",
    "X = sm.add_constant(x)\n",
    "\n",
    "# keep track of the variables in the model\n",
    "variables = list(x.columns)\n",
    "\n",
    "# Define the dummy groups\n",
    "dummy_group = {'embark': [], 'class': []}\n",
    "for var in variables:\n",
    "    # let's initialize the dummy group\n",
    "    for key in dummy_group.keys():\n",
    "        if var.startswith(key):\n",
    "            dummy_group[key].append(var)\n",
    "\n",
    "print(f\"variables: {variables}\")\n",
    "print(f\"dummy_group: {dummy_group}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the full model\n",
    "model = sm.Logit(y, X).fit()\n",
    "\n",
    "# Get the highest pvalues from the model\n",
    "pvalues = model.pvalues.drop('const')\n",
    "max_pvalue = pvalues.max()\n",
    "worst_var = pvalues.idxmax()\n",
    "\n",
    "# if the worst variable is not significant, we will remove it\n",
    "if max_pvalue > 0.05:\n",
    "    prefix = worst_var.split(\"_\")[0]\n",
    "    \n",
    "    # check if the worst variable is a dummy group    \n",
    "    if prefix in dummy_group:\n",
    "        # identify the dummy group that the worst_var belongs to\n",
    "        vars_to_drop = dummy_group[prefix]\n",
    "\n",
    "        # Compare the reduced model against the full model\n",
    "        X_reduced = X.drop(columns=vars_to_drop)\n",
    "        model_reduced = sm.Logit(y, X_reduced).fit()\n",
    "        \n",
    "        #TODO: Evaluate the significance of the reduced model\n",
    "        lr_test = -2 * (model_reduced.llf - model.llf)\n",
    "        f_pvalue = stats.chi2.sf(lr_test, len(vars_to_drop))\n",
    "        \n",
    "        # if the reduced model is not significant, we will remove the dummy group\n",
    "        if f_pvalue >= 0.05:  \n",
    "            X = X_reduced\n",
    "            print(f\"the dummy group {vars_to_drop} is excluded with f-pvalue {f_pvalue:.3f}\")\n",
    "        # otherwise, we keep the dummy group in the model but exclude from pvalue evaluation\n",
    "        else: \n",
    "            print(f\"the dummy group {vars_to_drop} is kept in the model\")\n",
    "            variables.remove(vars_to_drop)\n",
    "    \n",
    "    # if the worst variable is not a dummy group, we will remove it\n",
    "    else: \n",
    "        X = X.drop(columns=worst_var)\n",
    "        print(f\"Variable {worst_var} is excluded with p-value {max_pvalue:.3f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"All variables are significant\")\n",
    "    print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Interpret the impact of a feature\n",
    "Instead of using the coefficients, we will be transform them into the odds ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coefficients from the model\n",
    "b_coef = model.params\n",
    "\n",
    "# Exponentiate the parameters to get the odds ratio\n",
    "import numpy as np\n",
    "odds_ratios = np.exp(b_coef).round(3)\n",
    "print(odds_ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: How to interpre the odds ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Prediction and accuracy of prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Predict with Logistic model\n",
    "- logit -> odds -> probability -> category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: get the logit\n",
    "logits = model.fittedvalues\n",
    "# step 2: convert the logit to odds\n",
    "odds = np.exp(logits)\n",
    "# step 3: convert the odds to probability\n",
    "y_prob = odds / (1 + odds)\n",
    "# step 4: convert the probability to binary outcome\n",
    "y_pred = (y_prob > 0.5).astype(int)\n",
    "\n",
    "# display in a dataframe\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        'logit': logits,\n",
    "        'odds': odds, \n",
    "        'probability': y_prob, \n",
    "        'prediction': y_pred\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Evaluation Metrics for Classification Models\n",
    "\n",
    "- **Accuracy**: The proportion of correct predictions (both true positives and true negatives) among the total number of predictions. It measures overall correctness.\n",
    "  \n",
    "- **Precision**: The proportion of true positive predictions among all positive predictions. It measures how many of the predicted positives are actually correct.\n",
    "  \n",
    "- **Recall (Sensitivity)**: The proportion of true positive predictions among all actual positives. It measures how many of the actual positives were correctly identified.\n",
    "  \n",
    "- **F1-Score**: The harmonic mean of precision and recall, providing a balance between the two metrics.\n",
    "  \n",
    "- **Confusion Matrix**: A table showing the counts of true positives, false positives, true negatives, and false negatives, providing a comprehensive view of classification performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install scikit-learn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Evaluate the accuracy of prediction\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "precision = precision_score(y, y_pred)\n",
    "recall = recall_score(y, y_pred)\n",
    "f1 = f1_score(y, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "print(f\"Precision: {precision:.2%}\")\n",
    "print(f\"Recall: {recall:.2%}\")\n",
    "print(f\"F1-Score: {f1:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# plot the confusion matrix\n",
    "def plot_confusion_matrix(\n",
    "        y: np.ndarray, \n",
    "        y_pred: np.ndarray, \n",
    "        normalize: str | None = None) -> None:\n",
    "    \"\"\"\n",
    "    Plot a confusion matrix with improved visualization.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y, y_pred, normalize=normalize)\n",
    "    sns.heatmap(cm, annot=True, cmap='Blues',\n",
    "                xticklabels=['Negative', 'Positive'],\n",
    "                yticklabels=['Negative', 'Positive'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "\n",
    "# plot the probability distribution \n",
    "def plot_prediction_distribution(y: np.ndarray, y_prob: np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    Plot the probability distribution of the predicted values.\n",
    "    \"\"\"\n",
    "    plt.hist(y_prob[y == 0], bins=50, density=True, alpha=0.5,\n",
    "             label='Died', color='red')\n",
    "    plt.hist(y_prob[y == 1], bins=50, density=True, alpha=0.5,\n",
    "             label='Survived', color='blue')\n",
    "    plt.axvline(0.5, color='black', linestyle='--', alpha=0.5)\n",
    "    plt.xlabel('Predicted probability')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(y, y_pred=y_pred, normalize='true')\n",
    "\n",
    "# plot the probability distribution\n",
    "plot_prediction_distribution(y, y_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Handle Time Series Data \n",
    "- step 0: load the data and pre-processing\n",
    "- Step 1: Plot the data and identify patterns\n",
    "- Step 2: Choose appropriate model \n",
    "- Step 3: Make predictions and evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "# %pip install scikit-learn\n",
    "\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Load the data and pre-process data \n",
    "sales_data = pd.read_csv('RetailSales.csv')\n",
    "\n",
    "# Explore the data\n",
    "sales_data.info()\n",
    "sales_data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column to 'sales'\n",
    "sales_data = sales_data.rename(columns={'MRTSSM4453USN': 'sales'})\n",
    "\n",
    "# Convert to datetime\n",
    "sales_data['DATE'] = pd.to_datetime(sales_data['DATE'])\n",
    "\n",
    "# Set datetime as index\n",
    "sales_data = sales_data.set_index('DATE')\n",
    "sales_data.index.freq = 'MS'\n",
    "sales_data.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: Decompose the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "sales_data.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decompose the time series data\n",
    "\n",
    "# plot the decomposed data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: Choose an appropriate model\n",
    "SARIMA (Seasonal AutoRegressive Integrated Moving Average) Model:\n",
    " - AR(p): AutoRegressive component - uses past values to predict future values\n",
    " - I(d): Integrated component - differencing to make the time series stationary\n",
    " - MA(q): Moving Average component - uses past forecast errors in the model\n",
    " - Seasonal component (P,D,Q,s): Captures seasonal patterns in the data\n",
    " - Parameters (p,d,q)(P,D,Q,s) must be specified when fitting the model\n",
    " - p: order of the autoregressive part\n",
    " - d: degree of differencing required for stationarity\n",
    " - q: order of the moving average part\n",
    " - P: seasonal autoregressive order\n",
    " - D: seasonal differencing order\n",
    " - Q: seasonal moving average order\n",
    " - s: length of the seasonal cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity and as an example, we will choose a basic set of parameters.\n",
    "\n",
    "# Create the model and fit data\n",
    "sales_sarima = SARIMAX(\n",
    "    sales_data, \n",
    "    order=(2, 1, 1), \n",
    "    seasonal_order=(1, 1, 1, 12)\n",
    "    ).fit()\n",
    "\n",
    "\n",
    "# Show the model summary \n",
    "print(sales_sarima.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3: Make Predictions\n",
    "Prediction is a generally term that can apply to within or out of data range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction and plot the data\n",
    "fitted_values = sales_sarima.get_prediction()\n",
    "sales_pred = fitted_values.predicted_mean\n",
    "\n",
    "# Evaluate the accuracy\n",
    "mse = mean_squared_error(sales_data, sales_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "\n",
    "# Visualize the prediction\n",
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(sales_data, label='Actual Sales')\n",
    "plt.plot(sales_pred, label='Predicted Sales')\n",
    "\n",
    "# # Get the confidence interval\n",
    "# conf_lower = fitted_values.conf_int()['lower sales']\n",
    "# conf_upper = fitted_values.conf_int()['upper sales']\n",
    "# plt.fill_between(x=sales_pred.index,\n",
    "#                  y1=conf_lower,\n",
    "#                  y2=conf_upper,\n",
    "#                  color='gray')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Make Forecasts\n",
    "Forecast tyically refers to beyond current time horizon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sales_data, label='Actual Sales')\n",
    "\n",
    "# Make forecast and plot the confidence interval\n",
    "forecast_results = sales_sarima.get_forecast(steps=36)\n",
    "forecast = forecast_results.predicted_mean\n",
    "plt.plot(forecast, label='forecast')\n",
    "\n",
    "# Confidence interval\n",
    "conf_lower = forecast_results.conf_int()['lower sales']\n",
    "conf_upper = forecast_results.conf_int()['upper sales']\n",
    "plt.fill_between(x=forecast.index,\n",
    "                 y1=conf_lower,\n",
    "                 y2=conf_upper,\n",
    "                 color='gray')\n",
    "plt.legend()\n",
    "plt.xlim(left=pd.to_datetime('2018-01-01'))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
